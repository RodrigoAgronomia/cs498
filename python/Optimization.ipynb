{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sbn\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import  tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, w_layers, pred_layers):\n",
    "\n",
    "        super().__init__()\n",
    "              \n",
    "        # Weaher variables\n",
    "        w_layers_list = nn.ModuleList()\n",
    "        for i, o, k, d in w_layers:\n",
    "            w_layer = nn.Sequential(\n",
    "                nn.Conv1d(i, o, k),\n",
    "                nn.AvgPool1d(2),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.BatchNorm1d(o),\n",
    "                nn.Dropout(d)\n",
    "            )\n",
    "            w_layers_list.append(w_layer)\n",
    "        w_layers_list.append(nn.AdaptiveAvgPool1d(1))\n",
    "        self.w_layers = nn.Sequential(*w_layers_list)\n",
    "\n",
    "        \n",
    "        # Management variables\n",
    "        pred_layers_list = nn.ModuleList()\n",
    "        for i, o, d in pred_layers:\n",
    "            pred_layer = nn.Sequential(\n",
    "                nn.Linear(i, o),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.BatchNorm1d(o),\n",
    "                nn.Dropout(d)\n",
    "            )\n",
    "            pred_layers_list.append(pred_layer)\n",
    "        pred_layers_list.append(nn.Linear(o, 1))\n",
    "        self.pred_layers = nn.Sequential(*pred_layers_list)\n",
    "        \n",
    "   \n",
    "    def forward(self, Ws):\n",
    "        \n",
    "        feat = self.w_layers(Ws).view(Ws.shape[0], -1)\n",
    "        pred = self.pred_layers(feat)\n",
    "        return (torch.tanh(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(w):\n",
    "    ws = np.array([[[5e4,50,50,5,100.0]]])\n",
    "    w = w / ws\n",
    "    w = np.moveaxis(w, 1, 2)\n",
    "    wd = np.linspace(-0.9,2.1,300)[None,None]\n",
    "    wd = wd.repeat(len(w), 0)\n",
    "    w = np.concatenate([w, wd], 1)\n",
    "    w = torch.tensor(w, dtype=torch.float, device = device)\n",
    "    return(w)\n",
    "\n",
    "def back_transform(w):\n",
    "    w = w[:,:-1].cpu().data.numpy()\n",
    "    w = np.moveaxis(w, 2, 1)\n",
    "    ws = np.array([[[5e4,50,50,5,100.0]]])\n",
    "    w = w * ws\n",
    "    return(w)\n",
    "\n",
    "\n",
    "def get_adv(x):\n",
    "    x_opt = x.clone()\n",
    "    x_opt.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam([x_opt], lr=0.01)\n",
    "\n",
    "    for i in trange(100):\n",
    "\n",
    "        # Limpa os gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Obtem o output\n",
    "        outputs = model(x_opt)\n",
    "\n",
    "        # Calcula a perda pela loss function\n",
    "        loss = -outputs.mean()\n",
    "\n",
    "        # Use an l2 penalty:\n",
    "        loss += torch.norm(x - x_opt, 2, dim = 1).mean()\n",
    "\n",
    "        # Obtem os gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Atualiza os par√¢metros\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clip to the valid range of values:\n",
    "        x_opt.data = torch.clamp(x_opt.data, 0, 1)\n",
    "        x_opt.data[:,-1] = x_opt[:,-1]\n",
    "\n",
    "    return(x_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0.0\n",
    "w_layers =  [[6,12,3,d],[12,15,5,d],[15,20,7,d],[20,25,5,d],[25,100,3,d]]\n",
    "pred_layers = [[100,50,d],[50,50,d], [50,25,d]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values used to scale the weather data:\n",
    "ws = np.array([[[5e4,50,50,5,100.0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf = pd.read_hdf('../data/PSCE_TILE.h5', key = 'SIM').set_index('SIM')\n",
    "ydf['Yield'] = (ydf.TWSO/2e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = '../../../Apsim_test/MASAGRO/DAYMET_TILE'\n",
    "pxy = np.stack(np.meshgrid(np.arange(40), np.arange(40)), -1).reshape(-1, 2)\n",
    "wfiles = [f'{wdir}/DAYMET_9584_{px:02d}_{py:02d}.csv' for px, py in pxy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wridx = []\n",
    "# for wfile in wfiles:   \n",
    "#     ridx = 10 * np.arange(8,10) + np.random.randint(0, 10, 2)\n",
    "#     wridx.append(ridx)\n",
    "# wridx = np.array(wridx)\n",
    "# np.save('../data/opt_idx.npy', wridx)\n",
    "wridx = np.load('../data/opt_idx.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22055c295b2445599f02aa8000b9d22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_SIMl = []\n",
    "save_files = []\n",
    "yobs = []\n",
    "for wfile, ridx in zip(tqdm(wfiles), wridx):   \n",
    "    site = os.path.basename(wfile).replace('.csv', '')\n",
    "    sydf = ydf.loc[site]\n",
    "    yobs.append(sydf.Yield.values[ridx])\n",
    "\n",
    "    w = pd.read_csv(wfile, skiprows = 13)\n",
    "    w.DAY = pd.to_datetime(w.DAY, format = '%Y%m%d').dt.date\n",
    "    for crop_start_date in sydf.SIM_DATE.values[ridx]:\n",
    "        cs_date = np.where(w.DAY == crop_start_date)[0][0]\n",
    "        wrng = slice(cs_date-90, cs_date+210)\n",
    "        w_SIMl.append(w.iloc[wrng].copy())\n",
    "        \n",
    "        plant_date = format(crop_start_date, '%Y%m%d')\n",
    "        save_file = wfile.replace('.csv', f'_{plant_date}.csv')\n",
    "        save_files.append(save_file)\n",
    "\n",
    "yobs = np.array(yobs).reshape(-1)\n",
    "w_seed = [w.iloc[:,[1,2,3,4,6]].values for w in w_SIMl]\n",
    "w_seed = transform(np.stack(w_seed))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_methods = ['none', 'rnd', 'adv']\n",
    "\n",
    "# yp = []\n",
    "# for REP in range(1, 3):\n",
    "#     for train_method in train_methods:\n",
    "#         for PCT in [1,5]:\n",
    "#             model = MyNet(w_layers, pred_layers)\n",
    "#             model = model.to(device)\n",
    "#             model_file_name = f'../data/model_cnn_{train_method}_{PCT}_{REP}.pth'\n",
    "    \n",
    "#             model.load_state_dict(torch.load(model_file_name, map_location=device))\n",
    "#             model.eval()\n",
    "            \n",
    "#             ypred = model(w_seed).data.cpu().numpy().reshape(-1)\n",
    "#             yp.append(np.stack([yobs, ypred]))\n",
    "\n",
    "# yp = np.stack(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_methods = ['none', 'rnd', 'adv']\n",
    "\n",
    "for REP in range(1, 6):\n",
    "    for train_method in train_methods:\n",
    "        for PCT in [1,5]:\n",
    "            model = MyNet(w_layers, pred_layers)\n",
    "            model = model.to(device)\n",
    "            model_file_name = f'../data/model_cnn_{train_method}_{PCT}_{REP}.pth'\n",
    "            print(model_file_name)\n",
    "    \n",
    "            model.load_state_dict(torch.load(model_file_name, map_location=device))\n",
    "            model.eval()\n",
    "            \n",
    "            w_adv = get_adv(w_seed)\n",
    "            w_adv = back_transform(w_adv)\n",
    "\n",
    "            for i, (sw_adv, w_SIM) in enumerate(zip(w_adv, w_SIMl)):\n",
    "                w_SIM = w_SIM.copy()\n",
    "                w_SIM.iloc[:,[1,2,3,4,6]] = sw_adv\n",
    "                w_SIM.VAP = np.clip(w_SIM.VAP, 0.06, 199.3)\n",
    "                w_SIM.IRRAD = np.clip(w_SIM.IRRAD, 0.0, 40000000)\n",
    "                \n",
    "                save_file = save_files[i].replace('.csv', f'_opt_{train_method}_{PCT}_{REP}.csv')\n",
    "                with open(save_file, 'w') as sf:\n",
    "                    with open(wfile) as f:\n",
    "                        for r in range(14):\n",
    "                            sf.writelines(f.readline())\n",
    "\n",
    "                w_SIM.DAY = pd.to_datetime(w_SIM.DAY).dt.strftime('%Y%m%d')\n",
    "                w_SIM.to_csv(save_file, na_rep = 'NaN', mode = 'a', float_format = '%.3f', header = False, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GEOANN]",
   "language": "python",
   "name": "conda-env-GEOANN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
